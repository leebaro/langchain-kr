{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750a91b5",
   "metadata": {},
   "source": [
    "# STORM ê°œë…ì„ ë„ì…í•œ ì—°êµ¬ë¥¼ ìœ„í•œ ë©€í‹° ì—ì´ì „íŠ¸\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì˜ ëª©ì ì€ LangGraphë¥¼ í™œìš©í•˜ì—¬ ì—°êµ¬ ìë™í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£¹ë‹ˆë‹¤. \n",
    "\n",
    "ì—°êµ¬ëŠ” ì¢…ì¢… ë¶„ì„ê°€ì—ê²Œ ìœ„ì„ë˜ëŠ” ë…¸ë™ ì§‘ì•½ì ì¸ ì‘ì—…ì…ë‹ˆë‹¤. AIëŠ” ì´ëŸ¬í•œ ì—°êµ¬ ê³¼ì •ì„ ì§€ì›í•  ìˆ˜ ìˆëŠ” ìƒë‹¹í•œ ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì‚¬ìš©ì ë§ì¶¤í˜• AI ê¸°ë°˜ ì—°êµ¬ ë° ë³´ê³ ì„œ ìƒì„± ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ê²½ëŸ‰ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì—¬ ì—°êµ¬ ê³¼ì •ì„ ë§ì¶¤í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ì—°êµ¬ ì£¼ì œë¥¼ ì œê³µí•˜ê³ , ì‹œìŠ¤í…œì€ ê° í•˜ìœ„ ì£¼ì œì— ì§‘ì¤‘í•˜ëŠ” AI ë¶„ì„ê°€ íŒ€ì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ê³¼ì •ì—ì„œ `Human-in-the-loop`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ê°€ ì‹œì‘ë˜ê¸° ì „ì— í•˜ìœ„ ì£¼ì œë¥¼ ì„¸ë¶„í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "[STORM ë…¼ë¬¸](https://arxiv.org/abs/2402.14207)ì— ë”°ë¥´ë©´, **ìœ ì‚¬í•œ ì£¼ì œ ì¡°íšŒ**ì™€ **ë‹¤ì–‘í•œ ê´€ì ì˜ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜**ì„ í†µí•´ ì°¸ê³  ì¶œì²˜ ì‚¬ìš© ë¹ˆë„ì™€ ì •ë³´ ë°€ë„ë¥¼ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "**ì£¼ë¡œ ë‹¤ë£¨ëŠ” ë‚´ìš©**\n",
    "- **LangGraphì˜ ì£¼ìš” í…Œë§ˆ**: Memory, Human-in-the-loop, Controllability\n",
    "- **ì—°êµ¬ ìë™í™”ì˜ ëª©í‘œ**: ì‚¬ìš©ì ë§ì¶¤í˜• ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•\n",
    "- **ì†ŒìŠ¤ ì„ íƒ**: ì—°êµ¬ë¥¼ ìœ„í•œ ì…ë ¥ ì†ŒìŠ¤ ì„ íƒ\n",
    "- **ê³„íš**: ì£¼ì œ ì œê³µ ë° AI ë¶„ì„ê°€ íŒ€ ìƒì„±\n",
    "- **LLM í™œìš©**: ì „ë¬¸ê°€ AIì™€ì˜ ì‹¬ì¸µ ì¸í„°ë·°\n",
    "- **ì—°êµ¬ ê³¼ì •**: ë³‘ë ¬ë¡œ ì •ë³´ ìˆ˜ì§‘ ë° ì¸í„°ë·° ìˆ˜í–‰\n",
    "- **ì¶œë ¥ í˜•ì‹**: ìµœì¢… ë³´ê³ ì„œë¡œ í†µí•©ëœ í†µì°°ë ¥\n",
    "- **ì„¤ì •**: í™˜ê²½ ì„¤ì • ë° API í‚¤ ì„¤ì •\n",
    "- **ë¶„ì„ê°€ ìƒì„±**: Human-In-The-Loopë¥¼ í†µí•œ ë¶„ì„ê°€ ìƒì„± ë° ê²€í† \n",
    "- **ì¸í„°ë·° ìˆ˜í–‰**: ì§ˆë¬¸ ìƒì„± ë° ë‹µë³€ ìˆ˜ì§‘\n",
    "- **ë³‘ë ¬ ì¸í„°ë·°**: Map-Reduceë¥¼ í†µí•œ ì¸í„°ë·° ë³‘ë ¬í™”\n",
    "- **ìµœì¢… ë³´ê³ ì„œ ì‘ì„±**: ë³´ê³ ì„œì˜ ì„œë¡  ë° ê²°ë¡  ì‘ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339db37",
   "metadata": {},
   "source": [
    "ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ë‹¤ìŒì˜ ì„¸ ê°€ì§€ í…Œë§ˆë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "- **Memory**\n",
    "- **Human-in-the-loop**\n",
    "- **Controllability**\n",
    "\n",
    "ì´ì œ ì´ëŸ¬í•œ ê°œë…ì„ ê²°í•©í•˜ì—¬ AIì˜ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì‘ìš© ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì¸ ì—°êµ¬ ìë™í™”ë¥¼ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì—°êµ¬ëŠ” ì¢…ì¢… ë¶„ì„ê°€ì—ê²Œ ìœ„ì„ë˜ëŠ” ë…¸ë™ ì§‘ì•½ì ì¸ ì‘ì—…ì…ë‹ˆë‹¤. AIëŠ” ì´ëŸ¬í•œ ì—°êµ¬ ê³¼ì •ì„ ì§€ì›í•  ìˆ˜ ìˆëŠ” ìƒë‹¹í•œ ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—°êµ¬ëŠ” ë§ì¶¤í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì›ì‹œ LLM ì¶œë ¥ì€ ì‹¤ì œ ì˜ì‚¬ ê²°ì • ì›Œí¬í”Œë¡œìš°ì— ì í•©í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë§ì¶¤í˜• AI ê¸°ë°˜ [ì—°êµ¬ ë° ë³´ê³ ì„œ ìƒì„±](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) ì›Œí¬í”Œë¡œìš°ëŠ” ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìœ ë§í•œ ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "![langgraph-storm-concept](./assets/langgraph-storm-concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c5b8ea",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41258fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dad8507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "CH17-LangGraph-Use-Cases\n"
     ]
    }
   ],
   "source": [
    "# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(\"CH17-LangGraph-Use-Cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889da53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ìµœì‹  ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "GPT4o = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=GPT4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017f222",
   "metadata": {},
   "source": [
    "## ë¶„ì„ê°€ ìƒì„±: Human-In-The-Loop\n",
    "\n",
    "- **ë¶„ì„ê°€ ìƒì„±**: `Human-In-The-Loop`ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„ê°€ë¥¼ ìƒì„±í•˜ê³  ê²€í† í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7f039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "\n",
    "# ë¶„ì„ê°€ì˜ ì†ì„±ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class Analyst(BaseModel):\n",
    "    # ì£¼ìš” ì†Œì† ì •ë³´\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the analyst.\",\n",
    "    )\n",
    "    # ì´ë¦„\n",
    "    name: str = Field(description=\"Name of the analyst.\")\n",
    "\n",
    "    # ì—­í• \n",
    "    role: str = Field(\n",
    "        description=\"Role of the analyst in the context of the topic.\",\n",
    "    )\n",
    "    # ì¤‘ì , ìš°ë ¤ ì‚¬í•­ ë° ë™ê¸°ì— ëŒ€í•œ ì„¤ëª…\n",
    "    description: str = Field(\n",
    "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    # ë¶„ì„ê°€ì˜ ì¸ì  ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ëŠ” ì†ì„±\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "# ë¶„ì„ê°€ë“¤ì˜ ì§‘í•©\n",
    "class Perspectives(BaseModel):\n",
    "    # ë¶„ì„ê°€ ëª©ë¡\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845a360",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì€ Analyst í´ë˜ìŠ¤ë¥¼ í†µí•´ ìƒì„±ëœ ë¶„ì„ê°€ë“¤ì˜ ì§‘í•©ì„ ì¶”ì í•˜ëŠ” ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dba2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ ì •ì˜\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    # ì—°êµ¬ ì£¼ì œ\n",
    "    topic: str\n",
    "    # ìƒì„±í•  ë¶„ì„ê°€ì˜ ìµœëŒ€ ìˆ˜\n",
    "    max_analysts: int\n",
    "    # ì‚¬ëŒ í”¼ë“œë°±\n",
    "    human_analyst_feedback: str\n",
    "    # ë¶„ì„ê°€ ëª©ë¡\n",
    "    analysts: List[Analyst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba448ac",
   "metadata": {},
   "source": [
    "## ë¶„ì„ê°€(Analyst) ìƒì„± ë…¸ë“œ ì •ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd092f4",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œëŠ” ë¶„ì„ê°€(Analyst) ìƒì„± ë…¸ë“œë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œëŠ” ì£¼ì–´ì§„ ì—°êµ¬ ì£¼ì œì— ëŒ€í•´ ë‹¤ì–‘í•œ ë¶„ì„ê°€ë¥¼ ìƒì„±í•˜ëŠ” ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ê° ë¶„ì„ê°€ëŠ” ê³ ìœ í•œ ì—­í• ê³¼ ì†Œì†ì„ ê°€ì§€ë©°, ì£¼ì œì— ëŒ€í•œ ì „ë¬¸ì ì¸ ê´€ì ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd7ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# ë¶„ì„ê°€ ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
    "\n",
    "Follow these instructions carefully:\n",
    "1. First, review the research topic:\n",
    "\n",
    "{topic}\n",
    "        \n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "        \n",
    "{human_analyst_feedback}\n",
    "    \n",
    "3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "                    \n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme.\"\"\"\n",
    "\n",
    "\n",
    "# ë¶„ì„ê°€ ìƒì„± ë…¸ë“œ\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    # LLMì— êµ¬ì¡°í™”ëœ ì¶œë ¥ í˜•ì‹ì„ ì ìš©\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # ë¶„ì„ê°€ ìƒì„±ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts,\n",
    "    )\n",
    "\n",
    "    # LLMì„ í˜¸ì¶œí•˜ì—¬ ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ ìƒì„±\n",
    "    analysts = structured_llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
    "    )\n",
    "\n",
    "    # ìƒì„±ëœ ë¶„ì„ê°€ ëª©ë¡ì„ ìƒíƒœì— ì €ì¥\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì í”¼ë“œë°± ë…¸ë“œ(ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•  ì˜ˆì •ì´ë¯€ë¡œ, ë‚´ìš©ì€ ë¹„ì›Œ ë‘ì–´ë„ ë¬´ë°©)\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ê¸° ìœ„í•œ ì¤‘ë‹¨ì  ë…¸ë“œ\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# ì¸ê°„ í”¼ë“œë°± ì—¬ë¶€ì— ë”°ë¼ ì›Œí¬í”Œë¡œìš°ì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\"ì›Œí¬í”Œë¡œìš°ì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22196430",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ìƒì„±\n",
    "\n",
    "ì´ì œ ë¶„ì„ê°€ ìƒì„± ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33720ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# ì—£ì§€ ì—°ê²°\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€: ì‚¬ëŒ í”¼ë“œë°±ì´ ìˆì„ ê²½ìš° ë‹¤ì‹œ ë¶„ì„ê°€ ìƒì„± ë…¸ë“œë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", should_continue, [\"create_analysts\", END]\n",
    ")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìƒì„±\n",
    "memory = MemorySaver()\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼(ì¤‘ë‹¨ì  ì„¤ì •)\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b96538",
   "metadata": {},
   "source": [
    "## ë¶„ì„ê°€ ìƒì„±ì„ ìœ„í•œ ê·¸ë˜í”„ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ddccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcreate_analysts\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='University of AI Research' name='Dr. Sumi Chang' role='AI Researcher' description='Dr. Chang focuses on exploring the theoretical differences between Modular RAG and Naive RAG. She is particularly interested in understanding the unique architectural choices that differentiate these two approaches and how these choices impact their performance in modular and scalable AI systems.'\n",
      "affiliation='TechCorp Innovations' name='Rajesh Mehta' role='Systems Engineer' description='Rajesh is focused on the practical applications and production benefits of Modular RAG over Naive RAG. He examines how Modular RAG can be integrated into existing tech infrastructures and its impact on efficiency, scalability, and adaptability in a production environment.'\n",
      "affiliation='Data-Driven Solutions' name='Lucy Thompson' role='Data Scientist' description=\"Lucy's interest lies in the data handling capabilities of Modular RAG versus Naive RAG. She explores how Modular RAG offers improved data management and retrieval efficiencies, which can lead to better insights and faster decision-making processes in complex data ecosystems.\"\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__interrupt__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# ë¶„ì„ê°€ ìˆ˜ ì„¤ì •\n",
    "max_analysts = 3\n",
    "\n",
    "# ì—°êµ¬ ì£¼ì œ ì„¤ì •\n",
    "topic = \"Modular RAG ê°€ ê¸°ì¡´ì˜ Naive RAG ì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì™€ production level ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ì \"\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„° ì„¤ì •\n",
    "inputs = {\n",
    "    \"topic\": topic,\n",
    "    \"max_analysts\": max_analysts,\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623117de",
   "metadata": {},
   "source": [
    "`__interrupt__` ê°€ ì¶œë ¥ë˜ë©´ ì¸ê°„ì˜ í”¼ë“œë°±ì„ ë°›ì„ ì¤€ë¹„ê°€ ëœ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ì•„ë˜ì˜ ìƒíƒœë¥¼ ê°€ì ¸ì™€ì„œ ì¸ê°„ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8fe74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ì˜ í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "state = graph.get_state(config)\n",
    "\n",
    "# ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ í™•ì¸\n",
    "print(state.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6ca4e",
   "metadata": {},
   "source": [
    "`update_state()` ë¥¼ í†µí•´ ì¸ê°„ í”¼ë“œë°±ì„ ì£¼ì…í•©ë‹ˆë‹¤. ì´ë•Œ `human_analyst_feedback` í‚¤ì— í”¼ë“œë°± ë‚´ìš©ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ `as_node` ì¸ìë¥¼ í†µí•´ í”¼ë“œë°±ì„ ë°›ì„ ë…¸ë“œë¥¼ ì§€ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efba1d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '0ef137e6-49cd-4347-8c9a-03ed61f83ad5',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1eff4b0b-1528-62ce-8002-4825093961f4'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ human_feedback ë…¸ë“œì˜ ì—­í•  ìˆ˜í–‰\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\n",
    "        \"human_analyst_feedback\": \"Add in someone named Teddy Lee from a startup to add an entrepreneur perspective\"\n",
    "    },\n",
    "    as_node=\"human_feedback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b22e9d",
   "metadata": {},
   "source": [
    "`None` ê°’ì„ ì…ë ¥ìœ¼ë¡œ ì£¼ê²Œ ë˜ë©´, ì´ì–´ì„œ ê·¸ë˜í”„ê°€ ì§„í–‰ë©ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- ì¬ê°œí•˜ê³ ì í•  ë•ŒëŠ” ì…ë ¥ì— `None` ê°’ì„ í• ë‹¹í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì¬ê°œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65011c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcreate_analysts\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='StartUp Magic' name='Teddy Lee' role='Entrepreneur Perspective' description=\"Teddy Lee is a founder of a startup focusing on AI-driven solutions. He is interested in understanding the practical benefits and advantages of Modular RAG over Naive RAG at a production level. Teddy's focus is on scalability, cost efficiency, and real-world application of these models in a competitive startup environment. He is particularly concerned with how Modular RAG could provide his startup with an edge over competitors in delivering more efficient and effective AI solutions.\"\n",
      "affiliation='Tech University' name='Dr. Julia Chen' role='Academic Researcher' description='Dr. Julia Chen is a professor focused on machine learning systems and architectures. Her research is centered around exploring the underlying mechanisms of Modular RAG compared to Naive RAG, and analyzing the theoretical benefits regarding complexity, accuracy, and flexibility. Dr. Chen aims to uncover whether the modular approach offers significant improvements in tasks such as information retrieval and natural language processing compared to its naive counterpart.'\n",
      "affiliation='Innovative AI Solutions Inc.' name='Jordan Nguyen' role='Industry Analyst' description='Jordan Nguyen works as an industry analyst specializing in AI technologies for enterprise-level applications. He is tasked with assessing the impact of adopting Modular RAG in established businesses, evaluating production benefits like efficiency, integration ease, and potential ROI improvements. His focus is on understanding how Modular RAG can transform existing workflows and offer strategic advantages for large organizations seeking to optimize their AI deployments.'\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__interrupt__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ì´ì–´ì„œ ì§„í–‰\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf393cce",
   "metadata": {},
   "source": [
    "ë‹¤ì‹œ `__interrupt__` ê°€ ì¶œë ¥ë˜ë©´ ì¸ê°„ì˜ í”¼ë“œë°±ì„ ë°›ì„ ì¤€ë¹„ê°€ ëœ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì „ì˜ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ ë‹¤ì‹œ ì¸ê°„ í”¼ë“œë°±ì„ ì œê³µí•˜ì—¬ ìƒì„±ëœ ë¶„ì„ê°€ì˜ í˜ë¥´ì†Œë‚˜ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ, ì¶”ê°€ í”¼ë“œë°±ì´ ì—†ì„ ê²½ìš° `None` ê°’ì„ í• ë‹¹í•˜ì—¬ ë¶„ì„ê°€ ìƒì„± ì‘ì—…ì„ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab512d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '0ef137e6-49cd-4347-8c9a-03ed61f83ad5',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1eff4b0b-8305-6c56-8004-381f54fc5262'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì¶”ê°€ í”¼ë“œë°±ì´ ì—†ì„ ê²½ìš° None ê°’ì„ í• ë‹¹í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "human_feedback_input = None\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ human_feedback ë…¸ë“œì˜ ì—­í•  ìˆ˜í–‰\n",
    "graph.update_state(\n",
    "    config, {\"human_analyst_feedback\": human_feedback_input}, as_node=\"human_feedback\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35618b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ì–´ì„œ ì§„í–‰\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710300b6",
   "metadata": {},
   "source": [
    "ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d759708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ ë¶„ì„ê°€ ìˆ˜: 3\n",
      "================================\n",
      "Name: Teddy Lee\n",
      "Role: Entrepreneur Perspective\n",
      "Affiliation: StartUp Magic\n",
      "Description: Teddy Lee is a founder of a startup focusing on AI-driven solutions. He is interested in understanding the practical benefits and advantages of Modular RAG over Naive RAG at a production level. Teddy's focus is on scalability, cost efficiency, and real-world application of these models in a competitive startup environment. He is particularly concerned with how Modular RAG could provide his startup with an edge over competitors in delivering more efficient and effective AI solutions.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Dr. Julia Chen\n",
      "Role: Academic Researcher\n",
      "Affiliation: Tech University\n",
      "Description: Dr. Julia Chen is a professor focused on machine learning systems and architectures. Her research is centered around exploring the underlying mechanisms of Modular RAG compared to Naive RAG, and analyzing the theoretical benefits regarding complexity, accuracy, and flexibility. Dr. Chen aims to uncover whether the modular approach offers significant improvements in tasks such as information retrieval and natural language processing compared to its naive counterpart.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Jordan Nguyen\n",
      "Role: Industry Analyst\n",
      "Affiliation: Innovative AI Solutions Inc.\n",
      "Description: Jordan Nguyen works as an industry analyst specializing in AI technologies for enterprise-level applications. He is tasked with assessing the impact of adopting Modular RAG in established businesses, evaluating production benefits like efficiency, integration ease, and potential ROI improvements. His focus is on understanding how Modular RAG can transform existing workflows and offer strategic advantages for large organizations seeking to optimize their AI deployments.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ì˜ ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# ìµœì¢… ìƒíƒœì—ì„œ ìƒì„±ëœ ë¶„ì„ê°€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "analysts = final_state.values.get(\"analysts\")\n",
    "\n",
    "# ìƒì„±ëœ ë¶„ì„ê°€ ìˆ˜ ì¶œë ¥\n",
    "print(f\"ìƒì„±ëœ ë¶„ì„ê°€ ìˆ˜: {len(analysts)}\", end=\"\\n================================\\n\")\n",
    "\n",
    "# ê° ë¶„ì„ê°€ì˜ í˜ë¥´ì†Œë‚˜ ì¶œë ¥\n",
    "for analyst in analysts:\n",
    "    print(analyst.persona)\n",
    "    print(\"- \" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61b611",
   "metadata": {},
   "source": [
    "`final_state.next` ëŠ” ê·¸ë˜í”„ì˜ ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ëª¨ë“  ì‘ì—…ì´ ë§ˆë¬´ë¦¬ ë˜ì—ˆê¸° ë•Œë¬¸ì— ë¹ˆ `tuple` ì´ ì¶œë ¥ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92144e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ì˜ ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "print(final_state.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e14e5c",
   "metadata": {},
   "source": [
    "## ì¸í„°ë·° ìˆ˜í–‰\n",
    "\n",
    "### ì§ˆë¬¸ ìƒì„±\n",
    "\n",
    "- ë¶„ì„ê°€ëŠ” ì „ë¬¸ê°€ì—ê²Œ ì§ˆë¬¸ì„ ì œì‹œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "089de179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "# ì¸í„°ë·° ìƒíƒœ ì •ì˜\n",
    "class InterviewState(MessagesState):\n",
    "    # ëŒ€í™” í„´ìˆ˜\n",
    "    max_num_turns: int\n",
    "    # ì†ŒìŠ¤ ë¬¸ì„œë¥¼ í¬í•¨í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
    "    context: Annotated[list, operator.add]\n",
    "    # ì§€ì •ëœ ë¶„ì„ê°€\n",
    "    analyst: Analyst\n",
    "    # ì¸í„°ë·° ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” ë¬¸ìì—´\n",
    "    interview: str\n",
    "    # ë³´ê³ ì„œ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸\n",
    "    sections: list\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ ì¿¼ë¦¬ ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd2a54",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œëŠ” ì¸í„°ë·° ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74a90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "        \n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "        \n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "\n",
    "# ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ ì •ì˜\n",
    "def generate_question(state: InterviewState):\n",
    "    # ìƒíƒœì—ì„œ ë¶„ì„ê°€ì™€ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # ì§ˆë¬¸ ìƒì„±\n",
    "    # ë¶„ì„ê°€ì˜ ëª©í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    # LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìƒì„±\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # ìƒíƒœì— ë©”ì‹œì§€ ê¸°ë¡\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064303de",
   "metadata": {},
   "source": [
    "## ë„êµ¬ ì •ì˜\n",
    "\n",
    "ì „ë¬¸ê°€ëŠ” ì—¬ëŸ¬ ì†ŒìŠ¤ë¡œë¶€í„° ì •ë³´ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì›¹ ë¬¸ì„œ ìŠ¤í¬ë˜í•‘, VectorDB, ì›¹ ê²€ìƒ‰, ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ ë“± ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Arxiv, Tavily ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592a8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ì„ ìœ„í•œ TavilySearch ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "tavily_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "# Arxiv ê²€ìƒ‰ì„ ìœ„í•œ ArxivRetriever ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    load_all_available_meta=True,\n",
    "    get_full_documents=True,\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "arxiv_search_results = arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")\n",
    "print(arxiv_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03485bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arxiv ë©”íƒ€ë°ì´í„° ì¶œë ¥\n",
    "arxiv_search_results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc31182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arxiv ë‚´ìš© ì¶œë ¥\n",
    "print(arxiv_search_results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6294ff",
   "metadata": {},
   "source": [
    "ë¬¸ì„œ ê²€ìƒ‰ê²°ê³¼ë¥¼ í¬ë§·íŒ… ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0475512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…\n",
    "formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "    [\n",
    "        f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "        for doc in arxiv_search_results\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc66263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_search_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439f615",
   "metadata": {},
   "source": [
    "## ë…¸ë“œ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d26cd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„±\n",
    "search_instructions = SystemMessage(\n",
    "    content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
    "        \n",
    "First, analyze the full conversation.\n",
    "\n",
    "Pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"ì›¹ ê²€ìƒ‰ì„ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
    "\n",
    "    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ ì§€ì •\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "# Arxiv ê²€ìƒ‰ ë…¸ë“œ ìƒì„±\n",
    "def search_arxiv(state: InterviewState):\n",
    "    \"\"\"Arxiv ê²€ìƒ‰ ë…¸ë“œ\"\"\"\n",
    "\n",
    "    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    try:\n",
    "        # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        arxiv_search_results = arxiv_retriever.invoke(\n",
    "            search_query.search_query,\n",
    "            load_max_docs=2,\n",
    "            load_all_available_meta=True,\n",
    "            get_full_documents=True,\n",
    "        )\n",
    "\n",
    "        # ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ ì§€ì •\n",
    "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "            [\n",
    "                f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "                for doc in arxiv_search_results\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\"context\": [formatted_search_docs]}\n",
    "    except Exception as e:\n",
    "        print(f\"Arxiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        return {\n",
    "            \"context\": [\"<Error>Arxiv ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.</Error>\"]\n",
    "        }\n",
    "\n",
    "\n",
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}. \n",
    "        \n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "        \n",
    "1. Use only the information provided in the context. \n",
    "        \n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "        \n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "        \n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
    "\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± í•¨ìˆ˜ ì •ì˜\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ë…¸ë“œ\"\"\"\n",
    "\n",
    "    # ìƒíƒœì—ì„œ ë¶„ì„ê°€ì™€ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # ë©”ì‹œì§€ë¥¼ ì „ë¬¸ê°€ì˜ ë‹µë³€ìœ¼ë¡œ ëª…ëª…\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    # ìƒíƒœì— ë©”ì‹œì§€ ì¶”ê°€\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "\n",
    "# ì¸í„°ë·° ì €ì¥ í•¨ìˆ˜ ì •ì˜\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"ì¸í„°ë·° ì €ì¥\"\"\"\n",
    "\n",
    "    # ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # ì¸í„°ë·°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    # ì¸í„°ë·° í‚¤ì— ì €ì¥\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "\n",
    "# ë©”ì‹œì§€ ë¼ìš°íŒ… í•¨ìˆ˜ ì •ì˜\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"ì§ˆë¬¸ê³¼ ë‹µë³€ ì‚¬ì´ì˜ ë¼ìš°íŒ…\"\"\"\n",
    "\n",
    "    # ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    # ì „ë¬¸ê°€ì˜ ë‹µë³€ ìˆ˜ í™•ì¸\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    # ì „ë¬¸ê°€ê°€ ìµœëŒ€ í„´ ìˆ˜ ì´ìƒ ë‹µë³€í•œ ê²½ìš° ì¢…ë£Œ\n",
    "    if num_responses >= max_num_turns:\n",
    "        return \"save_interview\"\n",
    "\n",
    "    # ì´ ë¼ìš°í„°ëŠ” ê° ì§ˆë¬¸-ë‹µë³€ ìŒ í›„ì— ì‹¤í–‰ë¨\n",
    "    # ë…¼ì˜ ì¢…ë£Œë¥¼ ì‹ í˜¸í•˜ëŠ” ë§ˆì§€ë§‰ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return \"save_interview\"\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "# ì„¸ì…˜ ì‘ì„± ì§€ì‹œì‚¬í•­\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "\n",
    "Your task is to create a detailed and comprehensive section of a report, thoroughly analyzing a set of source documents.\n",
    "This involves extracting key insights, elaborating on relevant points, and providing in-depth explanations to ensure clarity and understanding. Your writing should include necessary context, supporting evidence, and examples to enhance the reader's comprehension. Maintain a logical and well-organized structure, ensuring that all critical aspects are covered in detail and presented in a professional tone.\n",
    "\n",
    "Please follow these instructions:\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "        \n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "        \n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Comprehensive analysis (### header)\n",
    "d. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "\n",
    "6. For the Comprehensive analysis section:\n",
    "- Provide a detailed examination of the information from the source documents.\n",
    "- Break down complex ideas into digestible segments, ensuring a logical flow of ideas.\n",
    "- Use sub-sections where necessary to cover multiple perspectives or dimensions of the analysis.\n",
    "- Support your analysis with data, direct quotes, and examples from the source documents.\n",
    "- Clearly explain the relevance of each point to the overall focus of the report.\n",
    "- Use bullet points or numbered lists for clarity when presenting multiple related ideas.\n",
    "- Ensure the tone remains professional and objective, avoiding bias or unsupported opinions.\n",
    "- Aim for at least 800 words to ensure the analysis is thorough.\n",
    "\n",
    "7. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "8. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "        \n",
    "9. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\"\"\"\n",
    "\n",
    "\n",
    "# ì„¹ì…˜ ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ë…¸ë“œ\"\"\"\n",
    "\n",
    "    # ìƒíƒœì—ì„œ ì»¨í…ìŠ¤íŠ¸, ë¶„ì„ê°€ ê°€ì ¸ì˜¤ê¸°\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # ì„¹ì…˜ ì‘ì„±ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Use this source to write your section: {context}\")]\n",
    "    )\n",
    "\n",
    "    # ìƒíƒœì— ì„¹ì…˜ ì¶”ê°€\n",
    "    return {\"sections\": [section.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb721d",
   "metadata": {},
   "source": [
    "## ì¸í„°ë·° ê·¸ë˜í”„ ìƒì„±\n",
    "\n",
    "ì¸í„°ë·°ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê·¸ë˜í”„ë¥¼ ì •ì˜í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ë…¸ë“œ ë° ì—£ì§€ ì¶”ê°€\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_arxiv\", search_arxiv)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# íë¦„ ì„¤ì •\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_arxiv\")\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "interview_builder.add_edge(\"search_arxiv\", \"answer_question\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\", route_messages, [\"ask_question\", \"save_interview\"]\n",
    ")\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "# ì¸í„°ë·° ê·¸ë˜í”„ ìƒì„±\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(\n",
    "    run_name=\"Conduct Interviews\"\n",
    ")\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(interview_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ì„ê°€ ëª©ë¡ì—ì„œ ì²« ë²ˆì§¸ ë¶„ì„ê°€ ì„ íƒ\n",
    "analysts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6fe55",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# ì£¼ì œ ì„¤ì •\n",
    "topic = \"Modular RAG ê°€ ê¸°ì¡´ì˜ Naive RAG ì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì™€ production level ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ì \"\n",
    "\n",
    "# ì¸í„°ë·° ì‹œì‘ ë©”ì‹œì§€ ìƒì„±\n",
    "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
    "\n",
    "# ìŠ¤ë ˆë“œ ID ì„¤ì •\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=100,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(\n",
    "    interview_graph,\n",
    "    {\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 5},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ca13a",
   "metadata": {},
   "source": [
    "ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0750b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì™„ì„±ëœ ì¸í„°ë·° ì„¹ì…˜ ì¶œë ¥\n",
    "Markdown(interview_graph.get_state(config).values[\"sections\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interview_graph.get_state(config).values[\"sections\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0704d",
   "metadata": {},
   "source": [
    "## ì¸í„°ë·°ë¥¼ ë³‘ë ¬ë¡œ ì§„í–‰ (map-reduce)\n",
    "\n",
    "- ì¸í„°ë·°ëŠ” langgraph ì˜ `Send()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬í™”í•˜ë©°, ì´ëŠ” `map` ë‹¨ê³„ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
    "- ì¸í„°ë·° ê²°ê³¼ëŠ” `reduce` ë‹¨ê³„ì—ì„œ ë³´ê³ ì„œ ë³¸ë¬¸ì— í†µí•©ë©ë‹ˆë‹¤.\n",
    "- ìµœì¢… ë³´ê³ ì„œì— ì„œë¡ ê³¼ ê²°ë¡ ì„ ì‘ì„±í•˜ëŠ” ë§ˆì§€ë§‰ ë‹¨ê³„ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3dd6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# ResearchGraphState ìƒíƒœ ì •ì˜\n",
    "class ResearchGraphState(TypedDict):\n",
    "    # ì—°êµ¬ ì£¼ì œ\n",
    "    topic: str\n",
    "    # ìƒì„±í•  ë¶„ì„ê°€ì˜ ìµœëŒ€ ìˆ˜\n",
    "    max_analysts: int\n",
    "    # ì¸ê°„ ë¶„ì„ê°€ì˜ í”¼ë“œë°±\n",
    "    human_analyst_feedback: str\n",
    "    # ì§ˆë¬¸ì„ í•˜ëŠ” ë¶„ì„ê°€ ëª©ë¡\n",
    "    analysts: List[Analyst]\n",
    "    # Send() API í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸\n",
    "    sections: Annotated[list, operator.add]\n",
    "    # ìµœì¢… ë³´ê³ ì„œì˜ ì„œë¡ \n",
    "    introduction: str\n",
    "    # ìµœì¢… ë³´ê³ ì„œì˜ ë³¸ë¬¸ ë‚´ìš©\n",
    "    content: str\n",
    "    # ìµœì¢… ë³´ê³ ì„œì˜ ê²°ë¡ \n",
    "    conclusion: str\n",
    "    # ìµœì¢… ë³´ê³ ì„œ\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a83efe",
   "metadata": {},
   "source": [
    "### LangGraph ì˜ Send() í•¨ìˆ˜ ì‚¬ìš©\n",
    "\n",
    "ì•„ë˜ëŠ” langgraph ì˜ `Send()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸í„°ë·°ë¥¼ ë³‘ë ¬ë¡œ ì‹œì‘í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- [LangGraph Send()](https://langchain-ai.github.io/langgraph/concepts/low_level/#send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6035e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "# ëª¨ë“  ì¸í„°ë·°ë¥¼ ì‹œì‘\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    # ì‚¬ëŒì˜ í”¼ë“œë°± í™•ì¸\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
    "\n",
    "    # ë§Œì•½, ì‚¬ëŒì˜ í”¼ë“œë°±ì´ ìˆìœ¼ë©´ ë¶„ì„ê°€ ìƒì„±ìœ¼ë¡œ ëŒì•„ê°€ê¸°\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Send() í•¨ìˆ˜ë¥¼ í†µí•´ ì¸í„°ë·° ë³‘ë ¬ë¡œ ì‹œì‘\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        return [\n",
    "            Send(\n",
    "                \"conduct_interview\",\n",
    "                {\n",
    "                    \"analyst\": analyst,\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(\n",
    "                            content=f\"So you said you were writing an article on {topic}?\"\n",
    "                        )\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "            for analyst in state[\"analysts\"]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8984c7",
   "metadata": {},
   "source": [
    "### ë³´ê³ ì„œ ì‘ì„± ì •ì˜\n",
    "\n",
    "ë‹¤ìŒì€ ì¸í„°ë·° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ ì‘ì„± ê°€ì´ë“œë¼ì¸ì„ ì •ì˜í•˜ê³  ë³´ê³ ì„œ ì‘ì„± í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db04abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³´ê³ ì„œ ì‘ì„± ì§€ì‹œì‚¬í•­\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n",
    "\n",
    "{topic}\n",
    "\n",
    "You have a team of analysts. Each analyst has done two things:\n",
    "\n",
    "1. They conducted an interview with an expert on a specific sub-topic.\n",
    "2. They write up their finding into a memo.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. You will be given a collection of memos from your analysts.  \n",
    "2. Carefully review and analyze the insights from each memo.  \n",
    "3. Consolidate these insights into a detailed and comprehensive summary that integrates the central ideas from all the memos.  \n",
    "4. Organize the key points from each memo into the appropriate sections provided below, ensuring that each section is logical and well-structured.  \n",
    "5. Include all required sections in your report, using `### Section Name` as the header for each.  \n",
    "6. Aim for approximately 250 words per section, providing in-depth explanations, context, and supporting details.  \n",
    "\n",
    "**Sections to consider (including optional ones for greater depth):**\n",
    "\n",
    "- **Background**: Theoretical foundations, key concepts, and preliminary information necessary to understand the methodology and results.\n",
    "- **Related Work**: Overview of prior studies and how they compare or relate to the current research.\n",
    "- **Problem Definition**: A formal and precise definition of the research question or problem the paper aims to address.\n",
    "- **Methodology (or Methods)**: Detailed description of the methods, algorithms, models, data collection processes, or experimental setups used in the study.\n",
    "- **Implementation Details**: Practical details of how the methods or models were implemented, including software frameworks, computational resources, or parameter settings.\n",
    "- **Experiments**: Explanation of experimental protocols, datasets, evaluation metrics, procedures, and configurations employed to validate the methods.\n",
    "- **Results**: Presentation of experimental outcomes, often with statistical tables, graphs, figures, or qualitative analyses.\n",
    "\n",
    "To format your report:\n",
    "\n",
    "1. Use markdown formatting.\n",
    "2. Include no pre-amble for the report.\n",
    "3. Use no sub-heading.\n",
    "4. Start your report with a single title header: ## Insights\n",
    "5. Do not mention any analyst names in your report.\n",
    "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
    "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
    "8. List your sources in order and do not repeat.\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "Here are the memos from your analysts to build your report from:\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "\n",
    "# ë³´ê³ ì„œ ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_report(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # ì„¹ì…˜ì„ ìš”ì•½í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic, context=formatted_str_sections\n",
    "    )\n",
    "    report = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Write a report based upon these memos.\")]\n",
    "    )\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "\n",
    "# ì„œë¡ ê³¼ ê²°ë¡  ì‘ì„± ì§€ì‹œì‚¬í•­\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 200 words, crisply previewing (for introduction),  or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting.\n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header.\n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "\n",
    "# ì„œë¡  ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # ì„¹ì…˜ì„ ìš”ì•½í•˜ì—¬ ì„œë¡  ì‘ì„±\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    intro = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report introduction\")]\n",
    "    )\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "\n",
    "# ê²°ë¡  ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # ì„¹ì…˜ì„ ìš”ì•½í•˜ì—¬ ê²°ë¡  ì‘ì„±\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    conclusion = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report conclusion\")]\n",
    "    )\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "\n",
    "# ìµœì¢… ë³´ê³ ì„œ ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ ëª¨ì•„ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = (\n",
    "        state[\"introduction\"]\n",
    "        + \"\\n\\n---\\n\\n## Main Idea\\n\\n\"\n",
    "        + content\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + state[\"conclusion\"]\n",
    "    )\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55157d",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"]\n",
    ")\n",
    "\n",
    "# ì¸í„°ë·° ê²°ê³¼ ë³´ê³ ì„œ ì‘ì„±\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "\n",
    "# ë³´ê³ ì„œ ìµœì¢… ì •ë¦¬\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c60bf",
   "metadata": {},
   "source": [
    "## ê·¸ë˜í”„ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "`max_analysts`, `topic` ì„ ììœ ë¡­ê²Œ ë³€ê²½í•˜ì—¬ ì‹¤í–‰í•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥ ë°ì´í„° ì„¤ì •\n",
    "max_analysts = 3\n",
    "topic = \"Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.\"\n",
    "\n",
    "# config ì„¤ì •\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=30,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„° ì„¤ì •\n",
    "inputs = {\"topic\": topic, \"max_analysts\": max_analysts}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰: ì²« ë²ˆì§¸ ì¤‘ë‹¨ ì§€ì ê¹Œì§€\n",
    "invoke_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98b941",
   "metadata": {},
   "source": [
    "human_feedback ì„ ì¶”ê°€í•˜ì—¬ ë¶„ì„ê°€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ee8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆë¡œìš´ ë¶„ì„ê°€ ì¶”ê°€\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\"human_analyst_feedback\": \"Add Prof. Jeffrey Hinton as a head of AI analyst\"},\n",
    "    as_node=\"human_feedback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379689e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05feb4d",
   "metadata": {},
   "source": [
    "ì´ì œ ì‚¬ëŒì˜ í”¼ë“œë°±ì„ ì¢…ë£Œí•˜ê³  ê·¸ë˜í”„ë¥¼ ì¬ê°œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì¬ê°œ\n",
    "graph.update_state(config, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca31213",
   "metadata": {},
   "source": [
    "ìµœì¢… ì™„ì„±ëœ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# ê·¸ë˜í”„ì˜ ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# ìµœì¢… ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "report = final_state.values.get(\"final_report\")\n",
    "\n",
    "# ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œ ì¶œë ¥\n",
    "display(Markdown(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12db79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
