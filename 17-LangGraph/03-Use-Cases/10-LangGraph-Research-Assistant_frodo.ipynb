{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750a91b5",
   "metadata": {},
   "source": [
    "# STORM 개념을 도입한 연구를 위한 멀티 에이전트\n",
    "\n",
    "이 튜토리얼의 목적은 LangGraph를 활용하여 연구 자동화 시스템을 구축하는 방법에 대해서 다룹니다. \n",
    "\n",
    "연구는 종종 분석가에게 위임되는 노동 집약적인 작업입니다. AI는 이러한 연구 과정을 지원할 수 있는 상당한 잠재력을 가지고 있습니다. 이 튜토리얼에서는 사용자 맞춤형 AI 기반 연구 및 보고서 생성 워크플로우를 구축하는 방법을 다룹니다.\n",
    "\n",
    "이번 튜토리얼에서는 경량의 다중 에이전트 시스템을 구축하여 연구 과정을 맞춤화하는 것을 목표로 합니다. 사용자는 연구 주제를 제공하고, 시스템은 각 하위 주제에 집중하는 AI 분석가 팀을 생성합니다. \n",
    "\n",
    "이 과정에서 `Human-in-the-loop`를 사용하여 연구가 시작되기 전에 하위 주제를 세분화합니다.\n",
    "\n",
    "[STORM 논문](https://arxiv.org/abs/2402.14207)에 따르면, **유사한 주제 조회**와 **다양한 관점의 대화 시뮬레이션**을 통해 참고 출처 사용 빈도와 정보 밀도를 증가시킬 수 있습니다. \n",
    "\n",
    "**주로 다루는 내용**\n",
    "- **LangGraph의 주요 테마**: Memory, Human-in-the-loop, Controllability\n",
    "- **연구 자동화의 목표**: 사용자 맞춤형 연구 프로세스 구축\n",
    "- **소스 선택**: 연구를 위한 입력 소스 선택\n",
    "- **계획**: 주제 제공 및 AI 분석가 팀 생성\n",
    "- **LLM 활용**: 전문가 AI와의 심층 인터뷰\n",
    "- **연구 과정**: 병렬로 정보 수집 및 인터뷰 수행\n",
    "- **출력 형식**: 최종 보고서로 통합된 통찰력\n",
    "- **설정**: 환경 설정 및 API 키 설정\n",
    "- **분석가 생성**: Human-In-The-Loop를 통한 분석가 생성 및 검토\n",
    "- **인터뷰 수행**: 질문 생성 및 답변 수집\n",
    "- **병렬 인터뷰**: Map-Reduce를 통한 인터뷰 병렬화\n",
    "- **최종 보고서 작성**: 보고서의 서론 및 결론 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339db37",
   "metadata": {},
   "source": [
    "이번 튜토리얼에서는 다음의 세 가지 테마를 다룹니다.\n",
    "\n",
    "- **Memory**\n",
    "- **Human-in-the-loop**\n",
    "- **Controllability**\n",
    "\n",
    "이제 이러한 개념을 결합하여 AI의 가장 인기 있는 응용 분야 중 하나인 연구 자동화를 다루겠습니다. \n",
    "\n",
    "연구는 종종 분석가에게 위임되는 노동 집약적인 작업입니다. AI는 이러한 연구 과정을 지원할 수 있는 상당한 잠재력을 가지고 있습니다. 그러나 연구는 맞춤화가 필요합니다. 원시 LLM 출력은 실제 의사 결정 워크플로우에 적합하지 않은 경우가 많습니다.\n",
    "\n",
    "맞춤형 AI 기반 [연구 및 보고서 생성](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) 워크플로우는 이를 해결할 수 있는 유망한 방법입니다.\n",
    "\n",
    "![langgraph-storm-concept](./assets/langgraph-storm-concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c5b8ea",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d41258fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dad8507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH17-LangGraph-Use-Cases\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH17-LangGraph-Use-Cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "889da53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 최신 모델 가져오기\n",
    "GPT4o = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatGoogleGenerativeAI(model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017f222",
   "metadata": {},
   "source": [
    "## 분석가 생성: Human-In-The-Loop\n",
    "\n",
    "- **분석가 생성**: `Human-In-The-Loop`를 활용하여 분석가를 생성하고 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be7f039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# 분석가의 속성과 메타데이터를 정의하는 클래스\n",
    "class Analyst(BaseModel):\n",
    "    # 주요 소속 정보\n",
    "    affiliation: str = Field(None,\n",
    "        description=\"Primary affiliation of the analyst.\",\n",
    "    )\n",
    "    # 이름\n",
    "    name: str = Field(None,description=\"Name of the analyst.\")\n",
    "\n",
    "    # 역할\n",
    "    role: str = Field(None,\n",
    "        description=\"Role of the analyst in the context of the topic.\",\n",
    "    )\n",
    "    # 중점, 우려 사항 및 동기에 대한 설명\n",
    "    description: str = Field(None,\n",
    "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    # 분석가의 인적 정보를 문자열로 반환하는 속성\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "# 분석가들의 집합\n",
    "class Perspectives(BaseModel):\n",
    "    # 분석가 목록\n",
    "    analysts: List[Analyst] = Field(default_factory=list,\n",
    "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845a360",
   "metadata": {},
   "source": [
    "다음은 Analyst 클래스를 통해 생성된 분석가들의 집합을 추적하는 상태를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8dba2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 정의\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    # 연구 주제\n",
    "    topic: str\n",
    "    # 생성할 분석가의 최대 수\n",
    "    max_analysts: int\n",
    "    # 사람 피드백\n",
    "    human_analyst_feedback: str\n",
    "    # 분석가 목록\n",
    "    analysts: List[Analyst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba448ac",
   "metadata": {},
   "source": [
    "## 분석가(Analyst) 생성 노드 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd092f4",
   "metadata": {},
   "source": [
    "다음으로는 분석가(Analyst) 생성 노드를 정의하겠습니다.\n",
    "\n",
    "아래 코드는 주어진 연구 주제에 대해 다양한 분석가를 생성하는 로직을 구현합니다. 각 분석가는 고유한 역할과 소속을 가지며, 주제에 대한 전문적인 관점을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5cd7ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# 분석가 생성 프롬프트\n",
    "# analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
    "\n",
    "# Follow these instructions carefully:\n",
    "# 1. First, review the research topic:\n",
    "\n",
    "# {topic}\n",
    "        \n",
    "# 2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "        \n",
    "# {human_analyst_feedback}\n",
    "    \n",
    "# 3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "                    \n",
    "# 4. Pick the top {max_analysts} themes.\n",
    "\n",
    "# 5. Assign one analyst to each theme. For each analyst, you must define their name, role, affiliation, and a detailed description of their focus, concerns, and motives.\n",
    "\n",
    "# 6. The affiliation should be a short string, and the description should be a paragraph.\n",
    "\n",
    "# \"\"\"\n",
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
    "\n",
    "Follow these instructions carefully:\n",
    "1. First, review the research topic:\n",
    "\n",
    "{topic}\n",
    "        \n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "        \n",
    "{human_analyst_feedback}\n",
    "    \n",
    "3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "                    \n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme.\"\"\"\n",
    "\n",
    "\n",
    "# 분석가 생성 노드\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"분석가 페르소나를 생성하는 함수\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    # LLM에 구조화된 출력 형식을 적용\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # 분석가 생성을 위한 시스템 프롬프트 구성\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts,\n",
    "    )\n",
    "\n",
    "    # LLM을 호출하여 분석가 페르소나 생성\n",
    "    analysts = structured_llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
    "    )\n",
    "\n",
    "    # 생성된 분석가 목록을 상태에 저장\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "\n",
    "# 사용자 피드백 노드(상태 업데이트를 진행할 예정이므로, 내용은 비워 두어도 무방)\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"사용자 피드백을 받기 위한 중단점 노드\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# 인간 피드백 여부에 따라 워크플로우의 다음 단계를 결정하는 함수\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\"워크플로우의 다음 단계를 결정하는 함수\"\"\"\n",
    "\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22196430",
   "metadata": {},
   "source": [
    "## 그래프 생성\n",
    "\n",
    "이제 분석가 생성 그래프를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f33720ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# 그래프 생성\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "\n",
    "# 노드 추가\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# 엣지 연결\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "\n",
    "# 조건부 엣지 추가: 사람 피드백이 있을 경우 다시 분석가 생성 노드로 돌아갑니다.\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", should_continue, [\"create_analysts\", END]\n",
    ")\n",
    "\n",
    "# 메모리 생성\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 그래프 컴파일(중단점 설정)\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b96538",
   "metadata": {},
   "source": [
    "## 분석가 생성을 위한 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06ddccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation=None name='Efficiency Expert' role='Performance Analyst' description=None\n",
      "affiliation=None name='Scalability Guru' role='System Architect' description=None\n",
      "affiliation=None name='Customization Advocate' role='Solutions Engineer' description=None\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# 분석가 수 설정\n",
    "max_analysts = 3\n",
    "\n",
    "# 연구 주제 설정\n",
    "topic = \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\"\n",
    "\n",
    "# 입력 데이터 설정\n",
    "inputs = {\n",
    "    \"topic\": topic,\n",
    "    \"max_analysts\": max_analysts,\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623117de",
   "metadata": {},
   "source": [
    "`__interrupt__` 가 출력되면 인간의 피드백을 받을 준비가 된 것입니다.\n",
    "\n",
    "이제 아래의 상태를 가져와서 인간 피드백을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8fe74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# 그래프의 현재 상태 가져오기\n",
    "state = graph.get_state(config)\n",
    "\n",
    "# 다음 실행할 노드 확인\n",
    "print(state.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6ca4e",
   "metadata": {},
   "source": [
    "`update_state()` 를 통해 인간 피드백을 주입합니다. 이때 `human_analyst_feedback` 키에 피드백 내용을 저장합니다.\n",
    "\n",
    "또한 `as_node` 인자를 통해 피드백을 받을 노드를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "efba1d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '891468a7-cd59-4239-ae06-9e11f6534d5b',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1eff4446-b61e-6e4c-8002-c6c28c40bd57'}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 상태를 업데이트하여 human_feedback 노드의 역할 수행\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\n",
    "        \"human_analyst_feedback\": \"Add in someone named Teddy Lee from a startup to add an entrepreneur perspective\"\n",
    "    },\n",
    "    as_node=\"human_feedback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b22e9d",
   "metadata": {},
   "source": [
    "`None` 값을 입력으로 주게 되면, 이어서 그래프가 진행됩니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 재개하고자 할 때는 입력에 `None` 값을 할당하여 그래프를 재개합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65011c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation=None name='Dr. Anya Sharma' role='Academic Researcher' description=None\n",
      "affiliation=None name='Kenji Tanaka' role='Lead ML Engineer' description=None\n",
      "affiliation=None name='Teddy Lee' role='Startup Founder' description=None\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 이어서 진행\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf393cce",
   "metadata": {},
   "source": [
    "다시 `__interrupt__` 가 출력되면 인간의 피드백을 받을 준비가 된 것입니다.\n",
    "\n",
    "이전의 방식과 동일하게 다시 인간 피드백을 제공하여 생성된 분석가의 페르소나를 조정하는 것도 가능합니다.\n",
    "\n",
    "하지만, 추가 피드백이 없을 경우 `None` 값을 할당하여 분석가 생성 작업을 종료할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ab512d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '891468a7-cd59-4239-ae06-9e11f6534d5b',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1eff4446-e5d9-66fa-8004-3453455b4233'}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추가 피드백이 없을 경우 None 값을 할당하여 상태 업데이트\n",
    "human_feedback_input = None\n",
    "\n",
    "# 그래프 상태를 업데이트하여 human_feedback 노드의 역할 수행\n",
    "graph.update_state(\n",
    "    config, {\"human_analyst_feedback\": human_feedback_input}, as_node=\"human_feedback\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35618b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이어서 진행\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710300b6",
   "metadata": {},
   "source": [
    "최종 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d759708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 분석가 수: 3\n",
      "================================\n",
      "Name: Dr. Anya Sharma\n",
      "Role: Academic Researcher\n",
      "Affiliation: None\n",
      "Description: None\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Kenji Tanaka\n",
      "Role: Lead ML Engineer\n",
      "Affiliation: None\n",
      "Description: None\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Teddy Lee\n",
      "Role: Startup Founder\n",
      "Affiliation: None\n",
      "Description: None\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "# 그래프의 최종 상태 가져오기\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# 최종 상태에서 생성된 분석가 목록 가져오기\n",
    "analysts = final_state.values.get(\"analysts\")\n",
    "\n",
    "# 생성된 분석가 수 출력\n",
    "print(f\"생성된 분석가 수: {len(analysts)}\", end=\"\\n================================\\n\")\n",
    "\n",
    "# 각 분석가의 페르소나 출력\n",
    "for analyst in analysts:\n",
    "    print(analyst.persona)\n",
    "    print(\"- \" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61b611",
   "metadata": {},
   "source": [
    "`final_state.next` 는 그래프의 다음 실행할 노드를 나타냅니다. 여기서는 모든 작업이 마무리 되었기 때문에 빈 `tuple` 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f92144e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# 그래프의 다음 실행할 노드 상태 가져오기\n",
    "print(final_state.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e14e5c",
   "metadata": {},
   "source": [
    "## 인터뷰 수행\n",
    "\n",
    "### 질문 생성\n",
    "\n",
    "- 분석가는 전문가에게 질문을 제시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "089de179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "# 인터뷰 상태 정의\n",
    "class InterviewState(MessagesState):\n",
    "    # 대화 턴수\n",
    "    max_num_turns: int\n",
    "    # 소스 문서를 포함하는 컨텍스트 리스트\n",
    "    context: Annotated[list, operator.add]\n",
    "    # 지정된 분석가\n",
    "    analyst: Analyst\n",
    "    # 인터뷰 내용을 저장하는 문자열\n",
    "    interview: str\n",
    "    # 보고서 섹션 리스트\n",
    "    sections: list\n",
    "\n",
    "\n",
    "# 검색 쿼리 데이터 클래스 정의\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd2a54",
   "metadata": {},
   "source": [
    "다음으로는 인터뷰 질문을 생성하는 노드를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a74a90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "        \n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "        \n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "\n",
    "# 질문을 생성하는 노드 정의\n",
    "def generate_question(state: InterviewState):\n",
    "    # 상태에서 분석가와 메시지 가져오기\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 질문 생성\n",
    "    # 분석가의 목표를 기반으로 시스템 메시지 생성\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    # LLM을 사용하여 질문 생성\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 상태에 메시지 기록\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064303de",
   "metadata": {},
   "source": [
    "## 도구 정의\n",
    "\n",
    "전문가는 여러 소스로부터 정보를 병렬로 수집하여 질문에 답변합니다.\n",
    "\n",
    "웹 문서 스크래핑, VectorDB, 웹 검색, 위키피디아 검색 등 다양한 도구를 사용할 수 있습니다.\n",
    "\n",
    "이 튜토리얼에서는 Arxiv, Tavily 검색을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "592a8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 검색 도구 초기화\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# 웹 검색을 위한 TavilySearch 인스턴스 생성\n",
    "tavily_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd85cc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '), Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'), Document(metadata={'Published': '2025-02-04', 'Title': 'Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation', 'Authors': 'Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, Amit Sharma', 'Summary': \"We introduce Plan*RAG, a novel framework that enables structured multi-hop\\nreasoning in retrieval-augmented generation (RAG) through test-time reasoning\\nplan generation. While existing approaches such as ReAct maintain reasoning\\nchains within the language model's context window, we observe that this often\\nleads to plan fragmentation and execution failures. Our key insight is that by\\nisolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's\\nworking memory, we can enable (1) systematic exploration of reasoning paths,\\n(2) atomic subqueries enabling precise retrievals and grounding, and (3)\\nefficiency through parallel execution and bounded context window utilization.\\nMoreover, Plan*RAG's modular design allows it to be integrated with existing\\nRAG methods, thus providing a practical solution to improve current RAG\\nsystems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently\\nachieves improvements over recently proposed methods such as RQ-RAG and\\nSelf-RAG, while maintaining comparable computational costs.\", 'entry_id': 'http://arxiv.org/abs/2410.20753v2', 'published_first_time': '2024-10-28', 'comment': '19 pages, preprint', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.LG'], 'links': ['http://arxiv.org/abs/2410.20753v2', 'http://arxiv.org/pdf/2410.20753v2']}, page_content='Plan∗RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\\nPrakhar Verma † 1 Sukruta Prakash Midigeshi 2 Gaurav Sinha 2 Arno Solin 1\\nNagarajan Natarajan 2 Amit Sharma 2\\nAbstract\\nWe introduce Plan∗RAG, a novel framework\\nthat enables structured multi-hop reasoning in\\nretrieval-augmented generation (RAG) through\\ntest-time reasoning plan generation. While exist-\\ning approaches such as ReAct maintain reason-\\ning chains within the language model’s context\\nwindow, we observe that this often leads to plan\\nfragmentation and execution failures. Our key\\ninsight is that by isolating the reasoning plan as\\na directed acyclic graph (DAG) outside the LM’s\\nworking memory, we can enable (1) systematic\\nexploration of reasoning paths, (2) atomic sub-\\nqueries enabling precise retrievals and ground-\\ning, and (3) efficiency through parallel execution\\nand bounded context window utilization. More-\\nover, Plan∗RAG’s modular design allows it to\\nbe integrated with existing RAG methods, thus\\nproviding a practical solution to improve current\\nRAG systems. On standard multi-hop reasoning\\nbenchmarks, Plan∗RAG consistently achieves im-\\nprovements over recently proposed methods such\\nas RQ-RAG and Self-RAG, while maintaining\\ncomparable computational costs.\\n1. Introduction\\nRetrieval-Augmented Generation (RAG, Lewis et al., 2020;\\nPetroni et al., 2020; Guu et al., 2020) has emerged as a\\npromising approach for grounding language model (LM)\\nresponses in external knowledge. However, RAG systems\\nstruggle with multi-hop queries that require reasoning across\\nmultiple retrieved documents (Tang & Yang, 2024; Wei\\net al., 2022). A key challenge lies in the initial retrieval step,\\nwhich often fails to retrieve sufficient relevant documents\\ndue to the query’s lack of full contextual information (Ma\\net al., 2023). This limitation has been highlighted in recent\\nsurveys (Torfi et al., 2020; Zhao et al., 2023) as a funda-\\n† Work done during an internship with Microsoft Research.\\n1Aalto University, Finland 2Microsoft Research. Correspondence\\nto: <prakhar.verma@aalto.fi>, <amshar@microsoft.com>.\\nRAG\\nSelf-RAG\\nReAct\\n20\\n25\\n30\\n35\\n40\\n45\\nTest-time planning improves RAG\\n25.51\\n34.09\\n33.15\\n31.12\\n37.31\\n40.44\\nAccuracy (%)\\nVanilla\\nWith Plan∗\\nFigure 1. Plan∗RAG improves performance on the HotpotQA\\nbenchmark substantially compared to various existing RAG meth-\\nods, demonstrating the value of externalizing planning as a directed\\nacyclic graph (DAG) outside of the LLM’s context.\\nmental barrier to reliable AI systems, particularly given the\\nwidespread deployment of large language models (Brown\\net al., 2020) across critical domains. Consider the query:\\n“Rumble Fish was a novel by the author of the coming-of-age\\nnovel published in what year by Viking Press?” Answering\\nthis requires an iterative retrieval process: identifying the\\nRumble Fish’s author, connecting to their coming-of-age\\nnovel, and determining its publication year. Single-step\\nretrieval in RAG systems often fails in such cases, as it\\nmay retrieve documents about Rumble Fish’s author and\\nViking Press without recognizing the intermediate fact—\\nthe author’s coming-of-age novel—must first be established.\\nFurthermore, Leng et al. (2024); Shuster et al. (2021) demon-\\nstrate that even when relevant documents are retrieved, LMs\\nstruggle to reason across them due to fixed context win-\\ndows, leading to information loss and broken reasoning\\nchain. These limitations pose risks in critical domains such\\nas healthcare and finance (Pal et al., 2023; Zhao et al., 2024),\\nwhere accurate multi-step reasoning is essential.\\nRecent research has attempted to address these limita-\\ntions through structured reasoning frameworks. Chain-of-\\nThought (CoT) prompting (Wei et al., 2022) and systematic\\nquery decomposition (Patel et al., 2022) have introduced\\nexplicit reasoning steps, enabling more granular thought\\nprocesses and targeted retrievals. Building upon these foun-\\ndations, Yao et al. (2023) proposed ReAct—a framework\\nthat creates a reasoning chain ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "# Arxiv 검색을 위한 ArxivRetriever 인스턴스 생성\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    load_all_available_meta=True,\n",
    "    get_full_documents=True,\n",
    ")\n",
    "\n",
    "# 검색 결과 출력\n",
    "arxiv_search_results = arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")\n",
    "print(arxiv_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "03485bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-07-26',\n",
       " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
       " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang',\n",
       " 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.',\n",
       " 'entry_id': 'http://arxiv.org/abs/2407.21059v1',\n",
       " 'published_first_time': '2024-07-26',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'categories': ['cs.CL', 'cs.AI', 'cs.IR'],\n",
       " 'links': ['http://arxiv.org/abs/2407.21059v1',\n",
       "  'http://arxiv.org/pdf/2407.21059v1']}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arxiv 메타데이터 출력\n",
    "arxiv_search_results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0fc31182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n"
     ]
    }
   ],
   "source": [
    "# Arxiv 내용 출력\n",
    "print(arxiv_search_results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6294ff",
   "metadata": {},
   "source": [
    "문서 검색결과를 포맷팅 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0475512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 검색 결과를 포맷팅\n",
    "formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "    [\n",
    "        f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "        for doc in arxiv_search_results\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "abc66263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2312.10997v5\" date=\"2024-03-27\" authors=\"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Large Language Models (LLMs) showcase impressive capabilities but encounter\n",
      "challenges like hallucination, outdated knowledge, and non-transparent,\n",
      "untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\n",
      "emerged as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the generation,\n",
      "particularly for knowledge-intensive tasks, and allows for continuous knowledge\n",
      "updates and integration of domain-specific information. RAG synergistically\n",
      "merges LLMs' intrinsic knowledge with the vast, dynamic repositories of\n",
      "external databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing the Naive RAG,\n",
      "the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\n",
      "tripartite foundation of RAG frameworks, which includes the retrieval, the\n",
      "generation and the augmentation techniques. The paper highlights the\n",
      "state-of-the-art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG systems.\n",
      "Furthermore, this paper introduces up-to-date evaluation framework and\n",
      "benchmark. At the end, this article delineates the challenges currently faced\n",
      "and points out prospective avenues for research and development.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\n",
      "Wangc, and Haofen Wang a,c\n",
      "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "cCollege of Design and Innovation, Tongji University\n",
      "Abstract—Large Language Models (LLMs) showcase impres-\n",
      "sive capabilities but encounter challenges like hallucination,\n",
      "outdated knowledge, and non-transparent, untraceable reasoning\n",
      "processes. Retrieval-Augmented Generation (RAG) has emerged\n",
      "as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the\n",
      "generation, particularly for knowledge-intensive tasks, and allows\n",
      "for continuous knowledge updates and integration of domain-\n",
      "specific information. RAG synergistically merges LLMs’ intrin-\n",
      "sic knowledge with the vast, dynamic repositories of external\n",
      "databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing\n",
      "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
      "It meticulously scrutinizes the tripartite foundation of RAG\n",
      "frameworks, which includes the retrieval, the generation and the\n",
      "augmentation techniques. The paper highlights the state-of-the-\n",
      "art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG\n",
      "systems. Furthermore, this paper introduces up-to-date evalua-\n",
      "tion framework and benchmark. At the end, this article delineates\n",
      "the challenges currently faced and points out prospective avenues\n",
      "for research and development 1.\n",
      "Index Terms—Large language model, retrieval-augmented gen-\n",
      "eration, natural language processing, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE language models (LLMs) have achieved remark-\n",
      "able success, though they still face significant limitations,\n",
      "especially in domain-specific or knowledge-intensive tasks [1],\n",
      "notably producing “hallucinations” [2] when handling queries\n",
      "beyond their training data or requiring current information. To\n",
      "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      "enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calcu-\n",
      "lation. By referencing external knowledge, RAG effectively\n",
      "reduces the problem of generating factually incorrect content.\n",
      "Its integration into LLMs has resulted in widespread adoption,\n",
      "establishing RAG as a key technology in advancing chatbots\n",
      "and enhancing the suitability of LLMs for real-world applica-\n",
      "tions.\n",
      "RAG technology has rapidly developed in recent years, and\n",
      "the technology tree summarizing related research is shown\n",
      "Corresponding Author.Email:haofen.wang@tongji.edu.cn\n",
      "1Resources\n",
      "are\n",
      "available\n",
      "at\n",
      "https://github.com/Tongji-KGLLM/\n",
      "RAG-Survey\n",
      "in Figure 1. The development trajectory of RAG in the era\n",
      "of large models exhibits several distinct stage characteristics.\n",
      "Initially, RAG’s inception coincided with the rise of the\n",
      "Transformer architecture, focusing on enhancing language\n",
      "models by incorporating additional knowledge through Pre-\n",
      "Training Models (PTM). This early stage was characterized\n",
      "by foundational work aimed at refining pre-training techniques\n",
      "[3]–[5].The subsequent arrival of ChatGPT [6] marked a\n",
      "pivotal moment, with LLM demonstrating powerful in context\n",
      "learning (ICL) capabilities. RAG research shifted towards\n",
      "providing better information for LLMs to answer more com-\n",
      "plex and knowledge-intensive tasks during the inference stage,\n",
      "leading to rapid development in RAG studies. As research\n",
      "progressed, the enhancement of RAG was no longer limited\n",
      "to the inference stage but began to incorporate more with LLM\n",
      "fine-tuning techniques.\n",
      "The burgeoning field of RAG has experienced swift growth,\n",
      "yet it has not been accompanied by a systematic synthesis that\n",
      "could clarify its broader trajectory. Thi\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2410.20753v2\" date=\"2025-02-04\" authors=\"Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, Amit Sharma\"/>\n",
      "<Title>\n",
      "Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "We introduce Plan*RAG, a novel framework that enables structured multi-hop\n",
      "reasoning in retrieval-augmented generation (RAG) through test-time reasoning\n",
      "plan generation. While existing approaches such as ReAct maintain reasoning\n",
      "chains within the language model's context window, we observe that this often\n",
      "leads to plan fragmentation and execution failures. Our key insight is that by\n",
      "isolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's\n",
      "working memory, we can enable (1) systematic exploration of reasoning paths,\n",
      "(2) atomic subqueries enabling precise retrievals and grounding, and (3)\n",
      "efficiency through parallel execution and bounded context window utilization.\n",
      "Moreover, Plan*RAG's modular design allows it to be integrated with existing\n",
      "RAG methods, thus providing a practical solution to improve current RAG\n",
      "systems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently\n",
      "achieves improvements over recently proposed methods such as RQ-RAG and\n",
      "Self-RAG, while maintaining comparable computational costs.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Plan∗RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\n",
      "Prakhar Verma † 1 Sukruta Prakash Midigeshi 2 Gaurav Sinha 2 Arno Solin 1\n",
      "Nagarajan Natarajan 2 Amit Sharma 2\n",
      "Abstract\n",
      "We introduce Plan∗RAG, a novel framework\n",
      "that enables structured multi-hop reasoning in\n",
      "retrieval-augmented generation (RAG) through\n",
      "test-time reasoning plan generation. While exist-\n",
      "ing approaches such as ReAct maintain reason-\n",
      "ing chains within the language model’s context\n",
      "window, we observe that this often leads to plan\n",
      "fragmentation and execution failures. Our key\n",
      "insight is that by isolating the reasoning plan as\n",
      "a directed acyclic graph (DAG) outside the LM’s\n",
      "working memory, we can enable (1) systematic\n",
      "exploration of reasoning paths, (2) atomic sub-\n",
      "queries enabling precise retrievals and ground-\n",
      "ing, and (3) efficiency through parallel execution\n",
      "and bounded context window utilization. More-\n",
      "over, Plan∗RAG’s modular design allows it to\n",
      "be integrated with existing RAG methods, thus\n",
      "providing a practical solution to improve current\n",
      "RAG systems. On standard multi-hop reasoning\n",
      "benchmarks, Plan∗RAG consistently achieves im-\n",
      "provements over recently proposed methods such\n",
      "as RQ-RAG and Self-RAG, while maintaining\n",
      "comparable computational costs.\n",
      "1. Introduction\n",
      "Retrieval-Augmented Generation (RAG, Lewis et al., 2020;\n",
      "Petroni et al., 2020; Guu et al., 2020) has emerged as a\n",
      "promising approach for grounding language model (LM)\n",
      "responses in external knowledge. However, RAG systems\n",
      "struggle with multi-hop queries that require reasoning across\n",
      "multiple retrieved documents (Tang & Yang, 2024; Wei\n",
      "et al., 2022). A key challenge lies in the initial retrieval step,\n",
      "which often fails to retrieve sufficient relevant documents\n",
      "due to the query’s lack of full contextual information (Ma\n",
      "et al., 2023). This limitation has been highlighted in recent\n",
      "surveys (Torfi et al., 2020; Zhao et al., 2023) as a funda-\n",
      "† Work done during an internship with Microsoft Research.\n",
      "1Aalto University, Finland 2Microsoft Research. Correspondence\n",
      "to: <prakhar.verma@aalto.fi>, <amshar@microsoft.com>.\n",
      "RAG\n",
      "Self-RAG\n",
      "ReAct\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "Test-time planning improves RAG\n",
      "25.51\n",
      "34.09\n",
      "33.15\n",
      "31.12\n",
      "37.31\n",
      "40.44\n",
      "Accuracy (%)\n",
      "Vanilla\n",
      "With Plan∗\n",
      "Figure 1. Plan∗RAG improves performance on the HotpotQA\n",
      "benchmark substantially compared to various existing RAG meth-\n",
      "ods, demonstrating the value of externalizing planning as a directed\n",
      "acyclic graph (DAG) outside of the LLM’s context.\n",
      "mental barrier to reliable AI systems, particularly given the\n",
      "widespread deployment of large language models (Brown\n",
      "et al., 2020) across critical domains. Consider the query:\n",
      "“Rumble Fish was a novel by the author of the coming-of-age\n",
      "novel published in what year by Viking Press?” Answering\n",
      "this requires an iterative retrieval process: identifying the\n",
      "Rumble Fish’s author, connecting to their coming-of-age\n",
      "novel, and determining its publication year. Single-step\n",
      "retrieval in RAG systems often fails in such cases, as it\n",
      "may retrieve documents about Rumble Fish’s author and\n",
      "Viking Press without recognizing the intermediate fact—\n",
      "the author’s coming-of-age novel—must first be established.\n",
      "Furthermore, Leng et al. (2024); Shuster et al. (2021) demon-\n",
      "strate that even when relevant documents are retrieved, LMs\n",
      "struggle to reason across them due to fixed context win-\n",
      "dows, leading to information loss and broken reasoning\n",
      "chain. These limitations pose risks in critical domains such\n",
      "as healthcare and finance (Pal et al., 2023; Zhao et al., 2024),\n",
      "where accurate multi-step reasoning is essential.\n",
      "Recent research has attempted to address these limita-\n",
      "tions through structured reasoning frameworks. Chain-of-\n",
      "Thought (CoT) prompting (Wei et al., 2022) and systematic\n",
      "query decomposition (Patel et al., 2022) have introduced\n",
      "explicit reasoning steps, enabling more granular thought\n",
      "processes and targeted retrievals. Building upon these foun-\n",
      "dations, Yao et al. (2023) proposed ReAct—a framework\n",
      "that creates a reasoning chain \n",
      "</Content>\n",
      "</Document>\n"
     ]
    }
   ],
   "source": [
    "print(formatted_search_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439f615",
   "metadata": {},
   "source": [
    "## 노드 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d26cd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# 검색 쿼리 작성\n",
    "search_instructions = SystemMessage(\n",
    "    content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
    "        \n",
    "First, analyze the full conversation.\n",
    "\n",
    "Pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# 웹 검색 수행 함수 정의\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"웹 검색을 통한 문서 검색\"\"\"\n",
    "\n",
    "    # 검색 쿼리 생성\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    # 검색 수행\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # 검색 결과 형식 지정\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "# Arxiv 검색 노드 생성\n",
    "def search_arxiv(state: InterviewState):\n",
    "    \"\"\"Arxiv 검색 노드\"\"\"\n",
    "\n",
    "    # 검색 쿼리 생성\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    try:\n",
    "        # 검색 수행\n",
    "        arxiv_search_results = arxiv_retriever.invoke(\n",
    "            search_query.search_query,\n",
    "            load_max_docs=2,\n",
    "            load_all_available_meta=True,\n",
    "            get_full_documents=True,\n",
    "        )\n",
    "\n",
    "        # 검색 결과 형식 지정\n",
    "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "            [\n",
    "                f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "                for doc in arxiv_search_results\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\"context\": [formatted_search_docs]}\n",
    "    except Exception as e:\n",
    "        print(f\"Arxiv 검색 중 오류 발생: {str(e)}\")\n",
    "        return {\n",
    "            \"context\": [\"<Error>Arxiv 검색 결과를 가져오는데 실패했습니다.</Error>\"]\n",
    "        }\n",
    "\n",
    "\n",
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}. \n",
    "        \n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "        \n",
    "1. Use only the information provided in the context. \n",
    "        \n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "        \n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "        \n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
    "\n",
    "\n",
    "# 질문에 대한 답변 생성 함수 정의\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"질문에 대한 답변 생성 노드\"\"\"\n",
    "\n",
    "    # 상태에서 분석가와 메시지 가져오기\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # 질문에 대한 답변 생성\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 메시지를 전문가의 답변으로 명명\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    # 상태에 메시지 추가\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "\n",
    "# 인터뷰 저장 함수 정의\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"인터뷰 저장\"\"\"\n",
    "\n",
    "    # 메시지 가져오기\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 인터뷰를 문자열로 변환\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    # 인터뷰 키에 저장\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "\n",
    "# 메시지 라우팅 함수 정의\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"질문과 답변 사이의 라우팅\"\"\"\n",
    "\n",
    "    # 메시지 가져오기\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    # 전문가의 답변 수 확인\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    # 전문가가 최대 턴 수 이상 답변한 경우 종료\n",
    "    if num_responses >= max_num_turns:\n",
    "        return \"save_interview\"\n",
    "\n",
    "    # 이 라우터는 각 질문-답변 쌍 후에 실행됨\n",
    "    # 논의 종료를 신호하는 마지막 질문 가져오기\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return \"save_interview\"\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "# 세션 작성 지시사항\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "\n",
    "Your task is to create a detailed and comprehensive section of a report, thoroughly analyzing a set of source documents.\n",
    "This involves extracting key insights, elaborating on relevant points, and providing in-depth explanations to ensure clarity and understanding. Your writing should include necessary context, supporting evidence, and examples to enhance the reader's comprehension. Maintain a logical and well-organized structure, ensuring that all critical aspects are covered in detail and presented in a professional tone.\n",
    "\n",
    "Please follow these instructions:\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "        \n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "        \n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Comprehensive analysis (### header)\n",
    "d. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "\n",
    "6. For the Comprehensive analysis section:\n",
    "- Provide a detailed examination of the information from the source documents.\n",
    "- Break down complex ideas into digestible segments, ensuring a logical flow of ideas.\n",
    "- Use sub-sections where necessary to cover multiple perspectives or dimensions of the analysis.\n",
    "- Support your analysis with data, direct quotes, and examples from the source documents.\n",
    "- Clearly explain the relevance of each point to the overall focus of the report.\n",
    "- Use bullet points or numbered lists for clarity when presenting multiple related ideas.\n",
    "- Ensure the tone remains professional and objective, avoiding bias or unsupported opinions.\n",
    "- Aim for at least 800 words to ensure the analysis is thorough.\n",
    "\n",
    "7. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "8. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "        \n",
    "9. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\"\"\"\n",
    "\n",
    "\n",
    "# 섹션 작성 함수 정의\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"질문에 대한 답변 생성 노드\"\"\"\n",
    "\n",
    "    # 상태에서 컨텍스트, 분석가 가져오기\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # 섹션 작성을 위한 시스템 프롬프트 정의\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Use this source to write your section: {context}\")]\n",
    "    )\n",
    "\n",
    "    # 상태에 섹션 추가\n",
    "    return {\"sections\": [section.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb721d",
   "metadata": {},
   "source": [
    "## 인터뷰 그래프 생성\n",
    "\n",
    "인터뷰를 수행하는 그래프를 정의하고 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f74e1e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 노드 및 엣지 추가\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_arxiv\", search_arxiv)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# 흐름 설정\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_arxiv\")\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "interview_builder.add_edge(\"search_arxiv\", \"answer_question\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\", route_messages, [\"ask_question\", \"save_interview\"]\n",
    ")\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "# 인터뷰 그래프 생성\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(\n",
    "    run_name=\"Conduct Interviews\"\n",
    ")\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(interview_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bbf7293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(affiliation=None, name='Dr. Anya Sharma', role='Academic Researcher', description=None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분석가 목록에서 첫 번째 분석가 선택\n",
    "analysts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6fe55",
   "metadata": {},
   "source": [
    "## 그래프 실행\n",
    "\n",
    "이제 그래프를 실행하고 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0672f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, Dr. Sharma, thank you for your time today. My name is David Chen, and I'm an analyst looking to get a better grasp of the nuances between \"naive\" and \"modular\" RAG systems, especially concerning their practical advantages in production environments.\n",
      "\n",
      "You mentioned you're writing an article on this. To start, could you elaborate on what you consider to be the defining characteristics that differentiate a \"naive\" RAG implementation from a \"modular\" one? What are the key architectural differences we're talking about here?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. Modular RAG is an advanced form of Retrieval-Augmented Generation that leverages a modular design to separate and optimize various components of the system. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. Seamless Integration: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
      "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
      "Naive RAG established the groundwork for retrieval-augmented systems by combining document retrieval with language model generation. For example, in a question-answering task, RECALL ensures that a RAG system accurately incorporates all relevant points from retrieved documents into the generated answer. Vector databases play a crucial role in the operation of RAG systems, providing the infrastructure required for storing and retrieving high-dimensional embeddings of contextual information needed for LLMs. These embeddings capture the semantic and contextual meaning of unstructured data, enabling precise similarity searches that underpin the effectiveness of retrieval-augmented generation. By integrating retrieval into generation, RAG systems deliver more accurate and context-aware outputs, making them effective for applications requiring current or specialized knowledge.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2502.11228v1\" date=\"2025-02-16\" authors=\"Mohammad Reza Rezaei, Adji Bousso Dieng\"/>\n",
      "<Title>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that\n",
      "evaluates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\n",
      "effectiveness in multi-hop reasoning tasks. The framework achieves significant\n",
      "accuracy improvements over traditional single-step and multi-step RAG\n",
      "approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\n",
      "best baseline. The benefits of Vendi-RAG are even more pronounced as the number\n",
      "of retrieved documents increases. Finally, we evaluated Vendi-RAG across\n",
      "different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\n",
      "observed consistent improvements, demonstrating that the framework's advantages\n",
      "are model-agnostic.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And\n",
      "Quality Significantly Improves Retrieval\n",
      "Augmented Generation With LLMs\n",
      "Mohammad R. Rezaei1, 3 and Adji Bousso Dieng2, 3\n",
      "1Institute of Biomedical Engineering, University of Toronto\n",
      "2Department of Computer Science, Princeton University\n",
      "3Vertaix\n",
      "February 18, 2025\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that eval-\n",
      "uates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets—HotpotQA, MuSiQue, and 2WikiMultiHopQA—demonstrate Vendi-\n",
      "RAG’s effectiveness in multi-hop reasoning tasks. The framework achieves\n",
      "significant accuracy improvements over traditional single-step and multi-step\n",
      "RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA,\n",
      "+4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-\n",
      "RAG, the current best baseline. The benefits of Vendi-RAG are even more\n",
      "pronounced as the number of retrieved documents increases. Finally, we eval-\n",
      "uated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4,\n",
      "and GPT-4o-mini, and observed consistent improvements, demonstrating that\n",
      "the framework’s advantages are model-agnostic.\n",
      "Keywords: RAG, LLMs, Question Answering, NLP\n",
      ", Diversity, Vendi Scoring\n",
      "1\n",
      "Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a transformative framework\n",
      "for enhancing the performance of large language models (LLMs) in domain-specific\n",
      "tasks such as question-answering (QA). By retrieving relevant information from\n",
      "external sources beyond the training set, RAG enables LLMs to answer specialized\n",
      "1\n",
      "arXiv:2502.11228v1  [cs.CL]  16 Feb 2025\n",
      "Retrieval\n",
      "Query\n",
      "Answer\n",
      "LLM Judge \n",
      "Quality Check\n",
      "Rewritten  \n",
      "Query\n",
      "Final \n",
      "Answer\n",
      "Vendi Score\n",
      "Vendi Score\n",
      "Diversity  \n",
      "weight s\n",
      "Decision Block \n",
      "s > threshold Thr \n",
      "Iteration < N\n",
      "…\n",
      "Reasoning\n",
      "Figure 1: The process begins with an initial retrieval step, where a diverse set of\n",
      "documents is retrieved using the Vendi Score, ensuring broad semantic coverage.\n",
      "Next, leveraging a reasoning step to construct a coherent path to the final answer,\n",
      "the LLM generates an answer, which then undergoes quality assessment by an LLM\n",
      "judge. Based on the answer quality, the retriever is adjusted to balance diversity and\n",
      "relevance: high-quality answers limit the emphasis on diversity, while low-quality\n",
      "answers prompt the retriever to prioritize diversity more heavily. This adjustment\n",
      "is controlled by an adaptive parameter, s, which is updated over iterations. The\n",
      "process continues until the answer quality reaches an optimal threshold, denoted\n",
      "by Thr. Finally, the highest-quality responses and documents are selected, ensuring\n",
      "both diversity and accuracy.\n",
      "queries more effectively (Achiam et al., 2023; Team et al., 2023; Jiang et al., 2024).\n",
      "This approach has been particularly successful in single-hop QA, where a question\n",
      "can be answered using information from a single document Raiaan et al. (2024);\n",
      "Kwiatkowski et al. (2019). For instance, answering a question such as \"Which\n",
      "country is filmmaker Sembene Ousmane from?\" only requires retrieving relevant\n",
      "information from a single document containing this fact.\n",
      "However, multi-hop QA introduces sig\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2312.10997v5\" date=\"2024-03-27\" authors=\"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Large Language Models (LLMs) showcase impressive capabilities but encounter\n",
      "challenges like hallucination, outdated knowledge, and non-transparent,\n",
      "untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\n",
      "emerged as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the generation,\n",
      "particularly for knowledge-intensive tasks, and allows for continuous knowledge\n",
      "updates and integration of domain-specific information. RAG synergistically\n",
      "merges LLMs' intrinsic knowledge with the vast, dynamic repositories of\n",
      "external databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing the Naive RAG,\n",
      "the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\n",
      "tripartite foundation of RAG frameworks, which includes the retrieval, the\n",
      "generation and the augmentation techniques. The paper highlights the\n",
      "state-of-the-art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG systems.\n",
      "Furthermore, this paper introduces up-to-date evaluation framework and\n",
      "benchmark. At the end, this article delineates the challenges currently faced\n",
      "and points out prospective avenues for research and development.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\n",
      "Wangc, and Haofen Wang a,c\n",
      "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "cCollege of Design and Innovation, Tongji University\n",
      "Abstract—Large Language Models (LLMs) showcase impres-\n",
      "sive capabilities but encounter challenges like hallucination,\n",
      "outdated knowledge, and non-transparent, untraceable reasoning\n",
      "processes. Retrieval-Augmented Generation (RAG) has emerged\n",
      "as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the\n",
      "generation, particularly for knowledge-intensive tasks, and allows\n",
      "for continuous knowledge updates and integration of domain-\n",
      "specific information. RAG synergistically merges LLMs’ intrin-\n",
      "sic knowledge with the vast, dynamic repositories of external\n",
      "databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing\n",
      "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
      "It meticulously scrutinizes the tripartite foundation of RAG\n",
      "frameworks, which includes the retrieval, the generation and the\n",
      "augmentation techniques. The paper highlights the state-of-the-\n",
      "art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG\n",
      "systems. Furthermore, this paper introduces up-to-date evalua-\n",
      "tion framework and benchmark. At the end, this article delineates\n",
      "the challenges currently faced and points out prospective avenues\n",
      "for research and development 1.\n",
      "Index Terms—Large language model, retrieval-augmented gen-\n",
      "eration, natural language processing, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE language models (LLMs) have achieved remark-\n",
      "able success, though they still face significant limitations,\n",
      "especially in domain-specific or knowledge-intensive tasks [1],\n",
      "notably producing “hallucinations” [2] when handling queries\n",
      "beyond their training data or requiring current information. To\n",
      "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      "enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calcu-\n",
      "lation. By referencing external knowledge, RAG effectively\n",
      "reduces the problem of generating factually incorrect content.\n",
      "Its integration into LLMs has resulted in widespread adoption,\n",
      "establishing RAG as a key technology in advancing chatbots\n",
      "and enhancing the suitability of LLMs for real-world applica-\n",
      "tions.\n",
      "RAG technology has rapidly developed in recent years, and\n",
      "the technology tree summarizing related research is shown\n",
      "Corresponding Author.Email:haofen.wang@tongji.edu.cn\n",
      "1Resources\n",
      "are\n",
      "available\n",
      "at\n",
      "https://github.com/Tongji-KGLLM/\n",
      "RAG-Survey\n",
      "in Figure 1. The development trajectory of RAG in the era\n",
      "of large models exhibits several distinct stage characteristics.\n",
      "Initially, RAG’s inception coincided with the rise of the\n",
      "Transformer architecture, focusing on enhancing language\n",
      "models by incorporating additional knowledge through Pre-\n",
      "Training Models (PTM). This early stage was characterized\n",
      "by foundational work aimed at refining pre-training techniques\n",
      "[3]–[5].The subsequent arrival of ChatGPT [6] marked a\n",
      "pivotal moment, with LLM demonstrating powerful in context\n",
      "learning (ICL) capabilities. RAG research shifted towards\n",
      "providing better information for LLMs to answer more com-\n",
      "plex and knowledge-intensive tasks during the inference stage,\n",
      "leading to rapid development in RAG studies. As research\n",
      "progressed, the enhancement of RAG was no longer limited\n",
      "to the inference stage but began to incorporate more with LLM\n",
      "fine-tuning techniques.\n",
      "The burgeoning field of RAG has experienced swift growth,\n",
      "yet it has not been accompanied by a systematic synthesis that\n",
      "could clarify its broader trajectory. Thi\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "\n",
      "\n",
      "Based on my understanding from the documents, here's a breakdown:\n",
      "\n",
      "*   **Naive RAG:** This is the initial implementation of Retrieval-Augmented Generation, operating on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process [adasci.org/how-does-modular-rag-improve-upon-naive-rag/]. It combines information retrieval with natural language generation to produce responses to queries or prompts [www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag]. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model [adasci.org/how-does-modular-rag-improve-upon-naive-rag/]. It hinges on the straightforward similarity of chunks, resulting in poor performance when confronted with complex queries and chunks with substantial variability [1]. Its core framework is constituted by indexing, retrieval, and generation [1].\n",
      "*   **Modular RAG:** This is an advanced form of Retrieval-Augmented Generation that leverages a modular design to separate and optimize various components of the system [adasci.org/how-does-modular-rag-improve-upon-naive-rag/]. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules [adasci.org/how-does-modular-rag-improve-upon-naive-rag/]. It facilitates a highly reconfigurable framework [1]. It transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms [1]. Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses [adasci.org/how-does-modular-rag-improve-upon-naive-rag/].\n",
      "\n",
      "**Production Level Advantages of Modular RAG:**\n",
      "\n",
      "*   Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems [1].\n",
      "\n",
      "Sources:\n",
      "\n",
      "[1] http://arxiv.org/abs/2407.21059v1\n",
      "[2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:178\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m config \u001b[38;5;241m=\u001b[39m RunnableConfig(\n\u001b[1;32m     11\u001b[0m     recursion_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     12\u001b[0m     configurable\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: random_uuid()},\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 그래프 실행\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterview_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manalyst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43manalysts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_num_turns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;49;00m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1797\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1797\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1804\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langgraph/pregel/runner.py:302\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langgraph/pregel/runner.py:619\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m panic:\n\u001b[0;32m--> 619\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langgraph/pregel/executor.py:83\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langgraph/utils/runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langgraph/utils/runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[89], line 24\u001b[0m, in \u001b[0;36msearch_web\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 검색 쿼리 생성\u001b[39;00m\n\u001b[1;32m     23\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(SearchQuery)\n\u001b[0;32m---> 24\u001b[0m search_query \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msearch_instructions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 검색 수행\u001b[39;00m\n\u001b[1;32m     27\u001b[0m search_docs \u001b[38;5;241m=\u001b[39m tavily_search\u001b[38;5;241m.\u001b[39minvoke(search_query\u001b[38;5;241m.\u001b[39msearch_query)\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:5360\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5355\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5356\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5357\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:951\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    927\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    939\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    940\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[1;32m    941\u001b[0m         messages,\n\u001b[1;32m    942\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    949\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m    950\u001b[0m     )\n\u001b[0;32m--> 951\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:196\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/tenacity/__init__.py:330\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\n\u001b[1;32m    327\u001b[0m     f, functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__kwdefaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/tenacity/__init__.py:467\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/tenacity/__init__.py:368\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/tenacity/__init__.py:390\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/tenacity/__init__.py:470\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    472\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project/langchain-kr/.conda/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:190\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini",
      "\u001b[0mDuring task with name 'search_web' and id 'f7250476-1cb5-c06b-7841-a75a501dd293'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# 주제 설정\n",
    "topic = \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\"\n",
    "\n",
    "# 인터뷰 시작 메시지 생성\n",
    "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
    "\n",
    "# 스레드 ID 설정\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=100,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(\n",
    "    interview_graph,\n",
    "    {\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 100},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ca13a",
   "metadata": {},
   "source": [
    "마크다운 형식으로 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0750b48",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sections'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 완성된 인터뷰 섹션 출력\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m Markdown(\u001b[43minterview_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msections\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sections'"
     ]
    }
   ],
   "source": [
    "# 완성된 인터뷰 섹션 출력\n",
    "Markdown(interview_graph.get_state(config).values[\"sections\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interview_graph.get_state(config).values[\"sections\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0704d",
   "metadata": {},
   "source": [
    "## 인터뷰를 병렬로 진행 (map-reduce)\n",
    "\n",
    "- 인터뷰는 langgraph 의 `Send()` 함수를 사용하여 병렬화하며, 이는 `map` 단계에 해당합니다.\n",
    "- 인터뷰 결과는 `reduce` 단계에서 보고서 본문에 통합됩니다.\n",
    "- 최종 보고서에 서론과 결론을 작성하는 마지막 단계를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3dd6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# ResearchGraphState 상태 정의\n",
    "class ResearchGraphState(TypedDict):\n",
    "    # 연구 주제\n",
    "    topic: str\n",
    "    # 생성할 분석가의 최대 수\n",
    "    max_analysts: int\n",
    "    # 인간 분석가의 피드백\n",
    "    human_analyst_feedback: str\n",
    "    # 질문을 하는 분석가 목록\n",
    "    analysts: List[Analyst]\n",
    "    # Send() API 키를 포함하는 섹션 리스트\n",
    "    sections: Annotated[list, operator.add]\n",
    "    # 최종 보고서의 서론\n",
    "    introduction: str\n",
    "    # 최종 보고서의 본문 내용\n",
    "    content: str\n",
    "    # 최종 보고서의 결론\n",
    "    conclusion: str\n",
    "    # 최종 보고서\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a83efe",
   "metadata": {},
   "source": [
    "### LangGraph 의 Send() 함수 사용\n",
    "\n",
    "아래는 langgraph 의 `Send()` 함수를 사용하여 인터뷰를 병렬로 시작하는 함수입니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- [LangGraph Send()](https://langchain-ai.github.io/langgraph/concepts/low_level/#send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6035e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "# 모든 인터뷰를 시작\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    # 사람의 피드백 확인\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
    "\n",
    "    # 만약, 사람의 피드백이 있으면 분석가 생성으로 돌아가기\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # 그렇지 않으면 Send() 함수를 통해 인터뷰 병렬로 시작\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        return [\n",
    "            Send(\n",
    "                \"conduct_interview\",\n",
    "                {\n",
    "                    \"analyst\": analyst,\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(\n",
    "                            content=f\"So you said you were writing an article on {topic}?\"\n",
    "                        )\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "            for analyst in state[\"analysts\"]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8984c7",
   "metadata": {},
   "source": [
    "### 보고서 작성 정의\n",
    "\n",
    "다음은 인터뷰 내용을 바탕으로 보고서 작성 가이드라인을 정의하고 보고서 작성 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db04abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보고서 작성 지시사항\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n",
    "\n",
    "{topic}\n",
    "\n",
    "You have a team of analysts. Each analyst has done two things:\n",
    "\n",
    "1. They conducted an interview with an expert on a specific sub-topic.\n",
    "2. They write up their finding into a memo.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. You will be given a collection of memos from your analysts.  \n",
    "2. Carefully review and analyze the insights from each memo.  \n",
    "3. Consolidate these insights into a detailed and comprehensive summary that integrates the central ideas from all the memos.  \n",
    "4. Organize the key points from each memo into the appropriate sections provided below, ensuring that each section is logical and well-structured.  \n",
    "5. Include all required sections in your report, using `### Section Name` as the header for each.  \n",
    "6. Aim for approximately 250 words per section, providing in-depth explanations, context, and supporting details.  \n",
    "\n",
    "**Sections to consider (including optional ones for greater depth):**\n",
    "\n",
    "- **Background**: Theoretical foundations, key concepts, and preliminary information necessary to understand the methodology and results.\n",
    "- **Related Work**: Overview of prior studies and how they compare or relate to the current research.\n",
    "- **Problem Definition**: A formal and precise definition of the research question or problem the paper aims to address.\n",
    "- **Methodology (or Methods)**: Detailed description of the methods, algorithms, models, data collection processes, or experimental setups used in the study.\n",
    "- **Implementation Details**: Practical details of how the methods or models were implemented, including software frameworks, computational resources, or parameter settings.\n",
    "- **Experiments**: Explanation of experimental protocols, datasets, evaluation metrics, procedures, and configurations employed to validate the methods.\n",
    "- **Results**: Presentation of experimental outcomes, often with statistical tables, graphs, figures, or qualitative analyses.\n",
    "\n",
    "To format your report:\n",
    "\n",
    "1. Use markdown formatting.\n",
    "2. Include no pre-amble for the report.\n",
    "3. Use no sub-heading.\n",
    "4. Start your report with a single title header: ## Insights\n",
    "5. Do not mention any analyst names in your report.\n",
    "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
    "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
    "8. List your sources in order and do not repeat.\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "Here are the memos from your analysts to build your report from:\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "\n",
    "# 보고서 작성 함수 정의\n",
    "def write_report(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 최종 보고서 작성\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic, context=formatted_str_sections\n",
    "    )\n",
    "    report = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Write a report based upon these memos.\")]\n",
    "    )\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "\n",
    "# 서론과 결론 작성 지시사항\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 200 words, crisply previewing (for introduction),  or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting.\n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header.\n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "\n",
    "# 서론 작성 함수 정의\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 서론 작성\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    intro = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report introduction\")]\n",
    "    )\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "\n",
    "# 결론 작성 함수 정의\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 결론 작성\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    conclusion = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report conclusion\")]\n",
    "    )\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "\n",
    "# 최종 보고서 작성 함수 정의\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    # 모든 섹션을 모아 최종 보고서 작성\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = (\n",
    "        state[\"introduction\"]\n",
    "        + \"\\n\\n---\\n\\n## Main Idea\\n\\n\"\n",
    "        + content\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + state[\"conclusion\"]\n",
    "    )\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55157d",
   "metadata": {},
   "source": [
    "## 그래프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be0e8b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# 그래프 생성\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "# 노드 정의\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "# 엣지 정의\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"]\n",
    ")\n",
    "\n",
    "# 인터뷰 결과 보고서 작성\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "\n",
    "# 보고서 최종 정리\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "# 컴파일\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c60bf",
   "metadata": {},
   "source": [
    "## 그래프 실행\n",
    "\n",
    "이제 그래프를 실행하고 결과를 확인합니다.\n",
    "\n",
    "`max_analysts`, `topic` 을 자유롭게 변경하여 실행해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation=None name='Dr. Anya Sharma' role='RAG Architecture Specialist' description=None\n",
      "affiliation=None name='Ben Carter' role='Production Engineer' description=None\n",
      "affiliation=None name='Elena Rodriguez' role='Data Quality Analyst' description=None\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터 설정\n",
    "max_analysts = 3\n",
    "topic = \"Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.\"\n",
    "\n",
    "# config 설정\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=100,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# 입력 데이터 설정\n",
    "inputs = {\"topic\": topic, \"max_analysts\": max_analysts}\n",
    "\n",
    "# 그래프 실행: 첫 번째 중단 지점까지\n",
    "invoke_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98b941",
   "metadata": {},
   "source": [
    "human_feedback 을 추가하여 분석가를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c7ee8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '95ec65fe-ad79-45e4-8c0b-3d714553b537',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1eff4435-2398-6c46-8002-22dd0b1b055b'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 분석가 추가\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\"human_analyst_feedback\": \"Add Prof. Jeffrey Hinton as a head of AI analyst\"},\n",
    "    as_node=\"human_feedback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379689e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation=None name='Prof. Jeffrey Hinton' role='Head of AI Analyst' description=None\n",
      "affiliation=None name='RAG Deep Dive Analyst' role='Expert in RAG architectures' description=None\n",
      "affiliation=None name='Production Implementation Analyst' role='Focuses on the practical aspects of deploying RAG systems' description=None\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 그래프 실행\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05feb4d",
   "metadata": {},
   "source": [
    "이제 사람의 피드백을 종료하고 그래프를 재개합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8d3dfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '95ec65fe-ad79-45e4-8c0b-3d714553b537',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1eff4435-5973-6cb2-8004-f1cddb18c2ec'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 재개\n",
    "graph.update_state(config, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "invoke_graph(graph, None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca31213",
   "metadata": {},
   "source": [
    "최종 완성된 보고서를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b46b6ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# 그래프의 최종 상태 가져오기\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# 최종 보고서 가져오기\n",
    "report = final_state.values.get(\"final_report\")\n",
    "\n",
    "# 마크다운 형식으로 최종 보고서 출력\n",
    "display(Markdown(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4333b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12db79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
